{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to update modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%xmode verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import visualize from tools\n",
    "from soda_mmqc.tools.visualize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_checklist_visualization(\"fig-checklist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%xmode verbose\n",
    "check_specific_report_html(\"fig-checklist\", \"plot-gap-labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_check_data(\"fig-checklist\", \"stat-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(df['prompt'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompts[0]\n",
    "metric = \"semantic_similarity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_data = df.loc[\n",
    "    (df['prompt'] == prompt) &\n",
    "    (df['metric'] == metric) &\n",
    "    (df['aggregation_level'] == 'panel')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = plotting_data[['doi', 'figure_id', 'panel_id', 'task_scores']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_task_data = pd.DataFrame(columns=[\n",
    "    'doi', 'figure_id', 'panel_id', 'task', 'score', 'true_positives', 'false_positives', 'false_negatives'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, row in task_data.iterrows():\n",
    "    task_scores = row['task_scores']\n",
    "    for key, value in task_scores.items():\n",
    "        remapped_task_data.loc[len(remapped_task_data)] = {\n",
    "            'doi': row['doi'],\n",
    "            'figure_id': row['figure_id'],\n",
    "            'panel_id': row['panel_id'],\n",
    "            'task': key,\n",
    "            'score': value['score'],\n",
    "            'true_positives': value['true_positives'],\n",
    "            'false_positives': value['false_positives'],\n",
    "            'false_negatives': value['false_negatives']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_task_data.loc[remapped_task_data['task'] == 'panel_label', 'score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rows where panel_label score is less than 1.0\n",
    "bad_panels = remapped_task_data[remapped_task_data['task'] == 'panel_label'][remapped_task_data['score'] < 1.0]\n",
    "bad_panels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "px.box(remapped_task_data, x='task', y='score', color='task', title=f'{prompt} - {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
